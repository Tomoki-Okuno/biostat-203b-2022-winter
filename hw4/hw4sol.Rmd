---
title: "Biostat 203B Homework 4"
author: "Tomoki Okuno"
subtitle: Due Mar 18 @ 11:59PM
output:
  # ioslides_presentation: default
  html_document:
    toc: true
    toc_depth: 4
---

```{r, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

Display machine information:
```{r}
sessionInfo()
```
Load database libraries and the tidyverse frontend:
```{r}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(lubridate))
suppressPackageStartupMessages(library(miceRanger))
```

## Q1. Missing data

Through the Shiny app developed in HW3, we observe abundant missing values in the MIMIC-IV ICU cohort we created. In this question, we use multiple imputation to obtain a data set without missing values.

0. Read following tutorials on the R package miceRanger for imputation: <https://github.com/farrellday/miceRanger>, <https://cran.r-project.org/web/packages/miceRanger/vignettes/miceAlgorithm.html>.

    A more thorough book treatment of the practical imputation strategies is the book [*_Flexible Imputation of Missing Data_*](https://stefvanbuuren.name/fimd/) by Stef van Buuren. 
    
 **Solution:**
I Read the tutorial on miceRanger and some parts of the book by Stef van Buuren.

1. Explain the jargon MCAR, MAR, and MNAR.

**Solution:**  
These terminologies represent the missing data mechanism.

- MCAR stands for missing completely at random: Missingness is independent of any data. The type of data is rarely found in real data.
- MAR stands for missing at random. Missingness does not depend on the missing data but on the observed data. For instance, the older a person is, the more likely the data on income are to be missing. The probability of missing data is randomly generated conditional on the observed data.  
- MNAR stands for missing not at Random: Missingness depends on the missing data itself. For example, the lower your income, the less likely you may be to report your income.

2. Explain in a couple of sentences how the Multiple Imputation by Chained Equations (MICE) work.

**Solution:**  
While single imputation supplements a missing data with an imputed value as the true value, multiple imputation iterates this procedure until convergence to integrate the given estimates to take into account the uncertainty due to missing values. MICE, one of the most typical methods, starts with a random draw from the observed data, and imputes missing values several times (e.g., 5 or 10) by regressing each missing variable on the remaining variables under the assumption the missing data are MAR.

3. Perform a data quality check of the ICU stays data. Discard variables with substantial missingness, say >5000 `NA`s. Replace apparent data entry errors by `NA`s.

**Solution:**  
I deleted variables with more than 5000 `NA`s, which were `deathtime`, `edregtime`, `edouttime`, and `dod`. For the remaining numerical variables, values outside the range of [Q1 - 1.5 IQR, Q3 + 1.5 IQR] were replaced with `NA`s as outliers.
```{r}
# import ICU cohort data created in HW3
icu_cohort <- read_rds("../hw3/mimiciv_shiny/icu_cohort.rds")
# delete patients aged 18 since I followed the instruction to keep them in HW3
icu_cohort <- icu_cohort %>%
  filter(age_hadm > 18)
# check variables with more than 5000 missing values

colSums(is.na(icu_cohort))
        
icu_cohort %>%
  select_if(colSums(is.na(icu_cohort)) > 5000) %>%
  colnames()
# keep only variables with less than 5000 missing values
icu_cohort_discard <- icu_cohort %>%
  select_if(colSums(is.na(icu_cohort)) <= 5000) %>%
  print(width = Inf)
# make a function to replace outliers to `NA`s basd on IQR rule
outlier_to_na <- function(x) {
  qnt <- quantile(x, probs = c(.25, .75), na.rm = TRUE)
  H <- 1.5 * IQR(x, na.rm = TRUE)
  x[x < (qnt[1] - H)] <- NA
  x[x > (qnt[2] + H)] <- NA
  x
}
# replace outliers to `NA`s for numerical variables
icu_cohort_replace <- icu_cohort_discard %>% 
  mutate_if(is.numeric, outlier_to_na)
summary(icu_cohort_replace)
colSums(is.na(icu_cohort_replace))
```


4. Impute missing values by `miceRanger` (request $m=3$ data sets). This step is computational intensive. Make sure to save the imputation results as a file. Hint: Setting `max.depth=10` in the `miceRanger` function may cut some computing time.

**Solution:**  
I kept only variables we plan to use and use `miceRanger` to save three imputed data sets to a `icu_cohort_imputed.rds`.
```{r}
# demographics
demo_var = c("gender", "age_hadm", "marital_status", "ethnicity",
             "thirty_day_mort")
# lab measurements
lab_item <- c(50912, 50971, 50983, 50902, 50882, 
                51221, 51301, 50931, 50960, 50893)
lab_item = paste("lab", lab_item, sep = "")
# vitals
vital_item <- c(220045, 220181, 220179, 223761, 220210)
vital_item = paste("chart", vital_item, sep = "")

all_vars = c(demo_var, lab_item, vital_item)

#kept only variables we plan to use
icu_cohort_mice <- icu_cohort_replace %>%
  select(all_of(all_vars))
summary(icu_cohort_mice)

if (file.exists("icu_cohort_imputed.rds")) {
  icu_cohort_imputed <- read_rds("icu_cohort_imputed.rds")
} else {
  parTime <- system.time(
    icu_cohort_imputed <- miceRanger(
      icu_cohort_mice,
      m = 3,
      max.depth = 10,
      returnModels = FALSE,
      verbose = TRUE
    )
  )
  icu_cohort_imputed %>%
    write_rds("icu_cohort_imputed.rds")
}
```

5. Make imputation diagnostic plots and explain what they mean.

**Solution:** 
The imputation diagnostic plots are shown below. Imputation diagnostic plots help check whether the imputed distribution may be valid or not by comparing the original one for each variable.

Imputation diagnostic plots tell you how valid the imputations may be, how they are distributed, which variables were used to impute other variables, and so on. The red line is the density of the original, nonmissing data. The smaller, black lines are the density of the imputed values in each of the datasets. If these don’t match up, it’s not a problem, however it may tell you that your data was not Missing Completely at Random (MCAR).

```{r}
# plotDistributions(icu_cohort_imputed, vars = 'allNumeric')
# plotCorrelations(icu_cohort_imputed, vars = 'allNumeric')
# plotVarConvergence(icu_cohort_imputed, vars = 'allNumeric')
# plotModelError(icu_cohort_imputed,vars = 'allNumeric')
# plotVarImportance(icu_cohort_imputed)
# plotImputationVariance(icu_cohort_imputed, ncol = 2, widths = c(5,3))
```

6. Choose one of the imputed data sets to be used in Q2. This is **not** a good idea to use just one imputed data set or to average multiple imputed data sets. Explain in a couple of sentences what the correct Multiple Imputation strategy is.

**Solution:**  
The correct Multiple Imputation strategy is to pool completed versions of imputed data sets, that is, integrate $m$ parameter estimates, accounting for uncertainty (variation) among imputed values. The variance combines the within-imputation and between-imputation variance, The pooled estimates are unbiased and have the valid statistical properties unlike using one imputed data set or averaging multiple imputed data sets.
```{r}
library(mice)
# complete imputed data sets
icu_cohort_complete <- completeData(icu_cohort_imputed)
icu_cohort_choice <- icu_cohort_complete[[1]]
```

## Q2. Predicting 30-day mortality

Develop at least two analytic approaches for predicting the 30-day mortality of patients admitted to ICU using demographic information (gender, age, marital status, ethnicity), first lab measurements during ICU stay, and first vital measurements during ICU stay. For example, you can use (1) logistic regression (`glm()` function in base R or keras), (2) logistic regression with lasso penalty (glmnet or keras package), (3) random forest (randomForest package), or (4) neural network (keras package).

1. Partition data into 80% training set and 20% test set. Stratify partitioning according the 30-day mortality status.

**Solution:**  
I installed caret package to split data into 80% training set and 20% test set.
```{r}
library(caTools)
library(caret)
set.seed(3456)
# index <- createDataPartition(icu_cohort_choice$thirty_day_mort,
#                                   p = .8,
#                                   list = FALSE,
#                                   times = 1)
# training_set <- icu_cohort_choice[index, ]
# test_set <- icu_cohort_choice[-index, ]

split = sample.split(icu_cohort_choice$thirty_day_mort, SplitRatio = 0.8)
training_set = subset(icu_cohort_choice, split == TRUE)
test_set = subset(icu_cohort_choice, split == FALSE)

# training_set[, -c(1, 3:5)] = scale(training_set[, -c(1, 3:5)] )
# test_set[, -c(1, 3:5)] = scale(tstset[, -c(1, 3:5)] )
```

2. Train the models using the train set.
3. Compare model prediction performance on the test set.

**Solution:**  
**(1)** logistic regression, using `glm()`
```{r}
# fit the model
logstc_model <- glm(thirty_day_mort ~ ., training_set, family = binomial)
summary(logstc_model)
```
```{r}
# predict 30-day mortality using the model
prob_logstc <- predict(logstc_model, test_set, type = "response")
# translate probabilities to predictions
predict_logstc <- ifelse(prob_logstc > 0.5, TRUE, FALSE)
# make confusion matrix for the fitted value versus 30-day mortality in test
table(observed = test_set$thirty_day_mort, predicted = predict_logstc)
# compute accuracy 
cat('Test accuracy', mean(predict_logstc == test_set$thirty_day_mort))
```

**(2)** logistic regression with lasso penalty using glmnet package
```{r}
library(glmnet)
# define x and y
x <- model.matrix(thirty_day_mort ~ ., training_set)
y <- training_set$thirty_day_mort
# find optimal value of lambda (alpha=1 => lasso)
cv.out <- cv.glmnet(x, y, alpha = 1, family = "binomial", type.measure = "auc")
cv.out
# plot(cv.out)
# min value of lambda
lambda_min <- cv.out$lambda.min
# best value of lambda
lambda_1se <- cv.out$lambda.1se
# fit the model
coef(cv.out, s = lambda_1se)
```
```{r}
#get test data
x_test <- model.matrix(test_set$thirty_day_mort ~ ., test_set)
# predict 30-day mortality using the model
prob_lasso <- predict(cv.out, newx = x_test, s = lambda_1se, type = "response")
# translate probabilities to predictions
predict_lasso <- ifelse(prob_lasso > 0.5, TRUE, FALSE)
# make confusion matrix for the fitted value versus 30-day mortality in test
table(observed = test_set$thirty_day_mort, predicted = predict_lasso)
# compute accuracy 
cat('Test accuracy', mean(predict_lasso == test_set$thirty_day_mort))
```
**(3)** random forest (randomForest package)
```{r}
library(randomForest)

# character to factor
training_set2 = training_set %>%
  mutate_if(is.character, as.factor)
test_set2 = test_set %>%
  mutate_if(is.character, as.factor)

# thirty_day_mort logic to factor
training_set2 <- transform(
  training_set2,
  thirty_day_mort = as.factor(thirty_day_mort)
)
test_set2 <- transform(
  test_set2,
  thirty_day_mort = as.factor(thirty_day_mort)
)

model.rf <- randomForest(thirty_day_mort ~ ., data = training_set2, ntree = 500)
predict_rf <- predict(model.rf, test_set2)

table(observed = test_set$thirty_day_mort, predicted = predict_rf)
cat('Test accuracy', mean(predict_rf == test_set2$thirty_day_mort))
```
**(4)** neural network (keras package)
```{r}
library(keras)
library(tensorflow)

# install_keras(tensorflow=2.6) on console
# install_keras() on console
# library(reticulate)
# virtualenv_create("r-reticulate")
               
set.seed(528)
x_train <- scale(training_set[, -c(1, 3:5)])
y_train <- training_set$thirty_day_mort

x_test <- scale(test_set[, -c(1, 3:5)])
y_test <- test_set$thirty_day_mort

x_train <- array(as.numeric(x_train), dim = c(dim(x_train)[[1]], 19))
x_test <- array(as.numeric(x_test), dim = c(dim(x_test)[[1]], 19))

model <- keras_model_sequential()
model %>%
  layer_dense(units = 12, activation = 'selu', input_shape = c(19)) %>%
  layer_dense(units = 7, activation = 'relu')  %>%
  layer_dense(units = 1)
summary(model) 
```

```{r}
library(keras)
set.seed(528)
#icu_cohort_shuffle <- icu_cohort_q2[sample(nrow(icu_cohort_q2)), ]
x_train <- scale(training_set[, -5])
y_train <- training_set$thirty_day_mort

x_test <- scale(test_set[, -5])
y_test <- test_set$thirty_day_mort

cat(length(x_train), 'train sequences\n')
cat(length(x_test), 'test sequences\n')
cat(length(x_test), 'test sequences\n')
x_train <- pad_sequences(x_train, maxlen = maxlen)
x_test <- pad_sequences(x_test, maxlen = maxlen)
cat('x_train shape:', dim(x_train), '\n')
cat('x_test shape:', dim(x_test), '\n')
cat('Build model...\n')

model <- keras_model_sequential()
model %>%
  layer_embedding(input_dim = max_features, output_dim = 128) %>% 
  layer_lstm(units = 64, dropout = 0.2, recurrent_dropout = 0.2) %>% 
  layer_dense(units = 1, activation = 'sigmoid')

# Try using different optimizers and different optimizer configs
model %>% compile(
  loss = 'binary_crossentropy',
  optimizer = 'adam',
  metrics = c('accuracy')
)
summary(model)

cat('Train...\n')
system.time({
model %>% fit(
  x_train, y_train,
  batch_size = batch_size,
  epochs = 10,
  validation_data = list(x_test, y_test)
)
})

scores <- model %>% evaluate(
  x_test, y_test,
  # batch_size = batch_size
)

cat('Test score:', scores[[1]])
cat('Test accuracy', scores[[2]])

```
